{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(\"api/db.sqlite3\")\n",
    "df = pd.read_sql_query(\"select * from submissions limit 7000;\", conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 4)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_subs</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>Beautiful Home :)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>This was finished yesterday..</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>My roommate is kicking me out because having a...</td>\n",
       "      <td>So, I am not asking for advice, really...mores...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>This was this kitchen I grew up with in London...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>Anyone know this style of home?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>My view from home after a long day</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>That is how we make our home even cozier place ðŸ¥°ðŸ˜Ž</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>Should I continue the backslash around the cor...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>My first attempt at wall design. Purposely wen...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>Best way to hide wires?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  subreddit_subs  \\\n",
       "0      Home           10119   \n",
       "1      Home           10119   \n",
       "2      Home           10119   \n",
       "3      Home           10119   \n",
       "4      Home           10119   \n",
       "5      Home           10119   \n",
       "6      Home           10119   \n",
       "7      Home           10119   \n",
       "8      Home           10119   \n",
       "9      Home           10119   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Beautiful Home :)   \n",
       "1                      This was finished yesterday..   \n",
       "2  My roommate is kicking me out because having a...   \n",
       "3  This was this kitchen I grew up with in London...   \n",
       "4                    Anyone know this style of home?   \n",
       "5                 My view from home after a long day   \n",
       "6  That is how we make our home even cozier place ðŸ¥°ðŸ˜Ž   \n",
       "7  Should I continue the backslash around the cor...   \n",
       "8  My first attempt at wall design. Purposely wen...   \n",
       "9                            Best way to hide wires?   \n",
       "\n",
       "                                                text  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2  So, I am not asking for advice, really...mores...  \n",
       "3                                                     \n",
       "4                                                     \n",
       "5                                                     \n",
       "6                                                     \n",
       "7                                                     \n",
       "8                                                     \n",
       "9                                                     "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     \n",
       "1                                                     \n",
       "2    So, I am not asking for advice, really...mores...\n",
       "3                                                     \n",
       "4                                                     \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].str.replace('[^\\w\\s]',' ')\n",
    "df['clean_title'] = df['title'].str.replace('[^\\w\\s]',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_subs</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>Beautiful Home :)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Beautiful Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>This was finished yesterday..</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>This was finished yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>My roommate is kicking me out because having a...</td>\n",
       "      <td>So, I am not asking for advice, really...mores...</td>\n",
       "      <td>So  I am not asking for advice  really   mores...</td>\n",
       "      <td>My roommate is kicking me out because having a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>This was this kitchen I grew up with in London...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>This was this kitchen I grew up with in London...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>Anyone know this style of home?</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Anyone know this style of home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>My view from home after a long day</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>My view from home after a long day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>That is how we make our home even cozier place ðŸ¥°ðŸ˜Ž</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>That is how we make our home even cozier place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>Should I continue the backslash around the cor...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Should I continue the backslash around the cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>My first attempt at wall design. Purposely wen...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>My first attempt at wall design  Purposely wen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Home</td>\n",
       "      <td>10119</td>\n",
       "      <td>Best way to hide wires?</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Best way to hide wires</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  subreddit_subs  \\\n",
       "0      Home           10119   \n",
       "1      Home           10119   \n",
       "2      Home           10119   \n",
       "3      Home           10119   \n",
       "4      Home           10119   \n",
       "5      Home           10119   \n",
       "6      Home           10119   \n",
       "7      Home           10119   \n",
       "8      Home           10119   \n",
       "9      Home           10119   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Beautiful Home :)   \n",
       "1                      This was finished yesterday..   \n",
       "2  My roommate is kicking me out because having a...   \n",
       "3  This was this kitchen I grew up with in London...   \n",
       "4                    Anyone know this style of home?   \n",
       "5                 My view from home after a long day   \n",
       "6  That is how we make our home even cozier place ðŸ¥°ðŸ˜Ž   \n",
       "7  Should I continue the backslash around the cor...   \n",
       "8  My first attempt at wall design. Purposely wen...   \n",
       "9                            Best way to hide wires?   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  So, I am not asking for advice, really...mores...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "5                                                      \n",
       "6                                                      \n",
       "7                                                      \n",
       "8                                                      \n",
       "9                                                      \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  So  I am not asking for advice  really   mores...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "5                                                      \n",
       "6                                                      \n",
       "7                                                      \n",
       "8                                                      \n",
       "9                                                      \n",
       "\n",
       "                                         clean_title  \n",
       "0                                  Beautiful Home     \n",
       "1                      This was finished yesterday    \n",
       "2  My roommate is kicking me out because having a...  \n",
       "3  This was this kitchen I grew up with in London...  \n",
       "4                    Anyone know this style of home   \n",
       "5                 My view from home after a long day  \n",
       "6  That is how we make our home even cozier place     \n",
       "7  Should I continue the backslash around the cor...  \n",
       "8  My first attempt at wall design  Purposely wen...  \n",
       "9                            Best way to hide wires   "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set our features as description, and target as subreddit.  \n",
    "# Create a mass text.\n",
    "\n",
    "features = ['clean_text', 'clean_title', 'subreddit_subs'] \n",
    "target = 'subreddit'\n",
    "\n",
    "X = df[features]\n",
    "y = df[[target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# create tokenizer object\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "        \"\"\"Return the tokens\"\"\"\n",
    "        return [token.text for token in tokenizer(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(text):\n",
    "        \"\"\"Return the Lemmas\"\"\"\n",
    "        lemmas = []\n",
    "        doc = nlp(text)\n",
    "    \n",
    "        for token in doc: \n",
    "            if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
    "                lemmas.append(token.lemma_)\n",
    "    \n",
    "        return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[\"clean_text\"]\n",
    "\n",
    "# Instantiate vectorizer object\n",
    "tfidf = TfidfVectorizer(tokenizer=get_lemmas, min_df=0.025, max_df=.98, ngram_range=(1,2))\n",
    "#tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "# Create a vocabulary and get word counts per document\n",
    "dtm = tfidf.fit_transform(text) # Similiar to fit_predict\n",
    "\n",
    "# Get feature names to use as dataframe column headers\n",
    "#dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelle/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/neighbors/base.py:217: UserWarning: cannot use tree with sparse input: using brute force\n",
      "  warnings.warn(\"cannot use tree with sparse input: \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "         metric_params=None, n_jobs=None, n_neighbors=501, p=2, radius=1.0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit on TF-IDF Vectors\n",
    "size = 501\n",
    "model  = NearestNeighbors(n_neighbors=size, algorithm='ball_tree')\n",
    "model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = [\"\"\"Capes are not a great idea of you are a super hero trying to save the world.\"\"\"]\n",
    "\n",
    "#vec_user_input = tfidf.transform(user_input)\n",
    "#dist, subreddit_index = model.kneighbors(vec_user_input.todense())\n",
    "dist, subreddit_index = model.kneighbors(tfidf.transform(user_input).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      subreddit                                              title text  \\\n",
      "4107        aww                      This dad did not want a puppy        \n",
      "4076        aww  This is \"Frida\", she has saved 52 people so fa...        \n",
      "4075        aww  My autistic son hates his photo taken, so I le...        \n",
      "4074        aww                        Her reaction at the end :â€™)        \n",
      "4073        aww                 Puppy wants to nap with his friend        \n",
      "4072        aww                                          Baby duck        \n",
      "4071        aww  Osaka Aquarium just stepped up their gift shop...        \n",
      "4070        aww  Service pitbull training to protect his owner'...        \n",
      "4069        aww  Gorgeous grey wolf becomes a good boy when vis...        \n",
      "4068        aww  Mister Weez was always my big chonk. After a y...        \n",
      "4067        aww             Mittens! I told you, no more fighting.        \n",
      "4066        aww               I take it my mailman is a dog lover.        \n",
      "4065        aww  When you just canâ€™t believe that youâ€™re seeing...        \n",
      "4064        aww         It's nice to know that he takes care of me        \n",
      "4063        aww  Doggo's polite and subtle implication that he ...        \n",
      "4062        aww  Is this what people mean when they say their d...        \n",
      "4061        aww  So this is Eclipse. Every day she leaves her h...        \n",
      "4060        aww      Kittens and $2 thrifts store find = Priceless        \n",
      "4059        aww       Her reaction to seeing my face after our nap        \n",
      "4058        aww  Hey everyone! Back in July I posted our sweet ...        \n",
      "4057        aww  My aunts cat hates when she leaves so she guar...        \n",
      "4056        aww              She's in love with the new tiny human        \n",
      "4055        aww  Mary was kicked out for loving Jake. Married f...        \n",
      "4054        aww  I couldn't believe what I was seeing when I pu...        \n",
      "4053        aww  Service doggo walks on glass floor for the fir...        \n",
      "4052        aww        Smooches from his fren makes him very happy        \n",
      "4051        aww  My cat learned that the alarm sound means I wa...        \n",
      "4050        aww  Dad gets (pretend) vaccinated first so daughte...        \n",
      "4077        aww                       Proud boy finding best stick        \n",
      "4049        aww  Eating breakfast at a small cafe when this lit...        \n",
      "...         ...                                                ...  ...   \n",
      "3919  worldnews  The Wuhan coronavirus has hit Xinjiang, where ...        \n",
      "3918  worldnews  Leader of Hong Kong democracy group attacked w...        \n",
      "3917  worldnews  Canada wants clearer warnings on junk food. Th...        \n",
      "3916  worldnews  Hong Kong protesters call for 'Mulan' boycott ...        \n",
      "3915  worldnews  China Invents Rice That Can Grow in Salt Water...        \n",
      "3914  worldnews  UN votes resoundingly to reject Trump's recogn...        \n",
      "3913  worldnews                            Impeachment to go ahead        \n",
      "3912  worldnews  Trump is â€œworst perpetrator of false informati...        \n",
      "3911  worldnews  Mueller draft report says Trump \"helped Putin ...        \n",
      "3910  worldnews  France becomes first country in Europe to ban ...        \n",
      "3909  worldnews  Carnival slapped with a $20 million fine after...        \n",
      "3908  worldnews  Video sparks fears Hong Kong protesters being ...        \n",
      "3907  worldnews  Civil rights pioneer Viola Desmond, a black wo...        \n",
      "3921  worldnews  Trump leaked to the Russians that Israel succe...        \n",
      "3906  worldnews  Boris Johnson's Conservative party has receive...        \n",
      "3904  worldnews  Trump White House associate George Nader was c...        \n",
      "3903  worldnews  Cannabis May Pose a 'Long-Term Risk' to the Al...        \n",
      "3902  worldnews  Khashoggi was to disclose Saudi use of chemica...        \n",
      "3901  worldnews  Taiwan election â€“ Tsai Ing-wen wins second pre...        \n",
      "3900  worldnews  Volodymyr Zelenskiy, the comedian who last wee...        \n",
      "3899  worldnews  Father at centre of measles outbreak didn't va...        \n",
      "3898  worldnews  Trump defends abandoning Kurds by saying they ...        \n",
      "3897  worldnews  Hong Kong protesters rally against China's Uig...        \n",
      "3896  worldnews  US House approves Hong Kong Human Rights and D...        \n",
      "3895  worldnews  Mexico's president called off a White House vi...        \n",
      "3894  worldnews  Massive investigation reveals Ivanka's busines...        \n",
      "3893  worldnews  China tells Donald Trump there is an 'internat...        \n",
      "3892  worldnews  Student in Peru makes history by writing thesi...        \n",
      "3905  worldnews  DNA Test Shows Subwayâ€™s Oven-Roasted Chicken I...        \n",
      "4469        aww  According to his owner, \"He constantly thinks ...        \n",
      "\n",
      "      subreddit_subs  \n",
      "4107        23378767  \n",
      "4076        23378767  \n",
      "4075        23378767  \n",
      "4074        23378767  \n",
      "4073        23378767  \n",
      "4072        23378767  \n",
      "4071        23378767  \n",
      "4070        23378767  \n",
      "4069        23378767  \n",
      "4068        23378767  \n",
      "4067        23378767  \n",
      "4066        23378767  \n",
      "4065        23378767  \n",
      "4064        23378767  \n",
      "4063        23378767  \n",
      "4062        23378767  \n",
      "4061        23378767  \n",
      "4060        23378767  \n",
      "4059        23378767  \n",
      "4058        23378767  \n",
      "4057        23378767  \n",
      "4056        23378767  \n",
      "4055        23378767  \n",
      "4054        23378767  \n",
      "4053        23378767  \n",
      "4052        23378767  \n",
      "4051        23378767  \n",
      "4050        23378767  \n",
      "4077        23378767  \n",
      "4049        23378767  \n",
      "...              ...  \n",
      "3919        23070096  \n",
      "3918        23070096  \n",
      "3917        23070096  \n",
      "3916        23070096  \n",
      "3915        23070096  \n",
      "3914        23070096  \n",
      "3913        23070096  \n",
      "3912        23070096  \n",
      "3911        23070096  \n",
      "3910        23070096  \n",
      "3909        23070096  \n",
      "3908        23070096  \n",
      "3907        23070096  \n",
      "3921        23070096  \n",
      "3906        23070096  \n",
      "3904        23070096  \n",
      "3903        23070096  \n",
      "3902        23070096  \n",
      "3901        23070096  \n",
      "3900        23070096  \n",
      "3899        23070096  \n",
      "3898        23070096  \n",
      "3897        23070096  \n",
      "3896        23070096  \n",
      "3895        23070096  \n",
      "3894        23070096  \n",
      "3893        23070096  \n",
      "3892        23070096  \n",
      "3905        23070096  \n",
      "4469        23378767  \n",
      "\n",
      "[501 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "recommended_reddits = [df[['subreddit','title','text','subreddit_subs']].iloc[n] for n in subreddit_index]\n",
    "\n",
    "print(*recommended_reddits, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "knnPickle = 'model_pkl.sav'\n",
    "\n",
    "index_df = df[['subreddit']]\n",
    "\n",
    "pickle.dump((model, index_df, tfidf), open(knnPickle, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tinder', 'leagueoflegends', 'AskMen'}\n",
      "['AskMen', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'Tinder', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'Tinder', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'leagueoflegends', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'leagueoflegends', 'AskMen', 'AskMen', 'AskMen', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'AskMen', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'Tinder', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'AskMen', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'Tinder', 'AskMen']\n"
     ]
    }
   ],
   "source": [
    "# # load the model from disk\n",
    "loaded_model, loaded_index_df, loaded_tfidf = pickle.load(open(knnPickle, 'rb'))\n",
    "dist, indices = loaded_model.kneighbors(loaded_tfidf.transform(user_input).todense())\n",
    "recommended_reddits = [loaded_index_df.iloc[n]['subreddit'] for n in indices[0]]\n",
    "print(set(recommended_reddits))\n",
    "print(recommended_reddits)\n",
    "\n",
    "def uniq(input):\n",
    "  output = []\n",
    "  for x in input:\n",
    "    if x not in output:\n",
    "      output.append(x)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP (numpy updates)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
