{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get & Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7846, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect(\"db.sqlite3\")\n",
    "df = pd.read_sql_query(\"select * from submissions;\", conn)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>creator</th>\n",
       "      <th>description</th>\n",
       "      <th>version</th>\n",
       "      <th>keywords</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>license_name</th>\n",
       "      <th>size</th>\n",
       "      <th>size(bytes)</th>\n",
       "      <th>downloads</th>\n",
       "      <th>discussions</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>kernels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlg-ulb/creditcardfraud</td>\n",
       "      <td>Credit Card Fraud Detection</td>\n",
       "      <td>Anonymized credit card transactions labeled as...</td>\n",
       "      <td>Machine Learning Group - ULB</td>\n",
       "      <td>Context\\n---------\\n\\nIt is important that cre...</td>\n",
       "      <td>3</td>\n",
       "      <td>finance, machine learning, crime</td>\n",
       "      <td>2018-03-23 01:17:27</td>\n",
       "      <td>Database: Open Database, Contents: Database Co...</td>\n",
       "      <td>66MB</td>\n",
       "      <td>69155632</td>\n",
       "      <td>131731</td>\n",
       "      <td>40</td>\n",
       "      <td>3052086</td>\n",
       "      <td>3323</td>\n",
       "      <td>2121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tmdb/tmdb-movie-metadata</td>\n",
       "      <td>TMDB 5000 Movie Dataset</td>\n",
       "      <td>Metadata on ~5,000 movies from TMDb</td>\n",
       "      <td>The Movie Database (TMDb)</td>\n",
       "      <td>### Background\\nWhat can we say about the succ...</td>\n",
       "      <td>2</td>\n",
       "      <td>film</td>\n",
       "      <td>2017-09-28 01:09:12</td>\n",
       "      <td>Other (specified in description)</td>\n",
       "      <td>9MB</td>\n",
       "      <td>9747156</td>\n",
       "      <td>92202</td>\n",
       "      <td>53</td>\n",
       "      <td>630515</td>\n",
       "      <td>1601</td>\n",
       "      <td>1462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hugomathien/soccer</td>\n",
       "      <td>European Soccer Database</td>\n",
       "      <td>25k+ matches, players &amp; teams attributes for E...</td>\n",
       "      <td>Hugo Mathien</td>\n",
       "      <td>The ultimate Soccer database for data analysis...</td>\n",
       "      <td>10</td>\n",
       "      <td>europe, association football</td>\n",
       "      <td>2016-10-23 22:31:38</td>\n",
       "      <td>Database: Open Database, Contents: © Original ...</td>\n",
       "      <td>34MB</td>\n",
       "      <td>36121187</td>\n",
       "      <td>80922</td>\n",
       "      <td>95</td>\n",
       "      <td>638231</td>\n",
       "      <td>1875</td>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lava18/google-play-store-apps</td>\n",
       "      <td>Google Play Store Apps</td>\n",
       "      <td>Web scraped data of 10k Play Store apps for an...</td>\n",
       "      <td>Lavanya Gupta</td>\n",
       "      <td>### Context\\n\\nWhile many public datasets (on ...</td>\n",
       "      <td>6</td>\n",
       "      <td>internet, video games, computer science, mobil...</td>\n",
       "      <td>2019-02-03 13:55:47</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2MB</td>\n",
       "      <td>2013348</td>\n",
       "      <td>72020</td>\n",
       "      <td>37</td>\n",
       "      <td>440983</td>\n",
       "      <td>1861</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zynicide/wine-reviews</td>\n",
       "      <td>Wine Reviews</td>\n",
       "      <td>130k wine reviews with variety, location, wine...</td>\n",
       "      <td>zackthoutt</td>\n",
       "      <td>### Context\\n\\nAfter watching [Somm](http://ww...</td>\n",
       "      <td>4</td>\n",
       "      <td>food and drink, critical theory</td>\n",
       "      <td>2017-11-27 17:08:04</td>\n",
       "      <td>CC BY-NC-SA 4.0</td>\n",
       "      <td>51MB</td>\n",
       "      <td>53336173</td>\n",
       "      <td>68002</td>\n",
       "      <td>24</td>\n",
       "      <td>342243</td>\n",
       "      <td>1609</td>\n",
       "      <td>1948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ronitf/heart-disease-uci</td>\n",
       "      <td>Heart Disease UCI</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Heart+...</td>\n",
       "      <td>ronit</td>\n",
       "      <td>### Context \\n\\nThis database contains 76 attr...</td>\n",
       "      <td>1</td>\n",
       "      <td>classification, binary classification, health,...</td>\n",
       "      <td>2018-06-25 11:33:56</td>\n",
       "      <td>Reddit API Terms</td>\n",
       "      <td>3KB</td>\n",
       "      <td>3438</td>\n",
       "      <td>67832</td>\n",
       "      <td>41</td>\n",
       "      <td>404892</td>\n",
       "      <td>2161</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unsdsn/world-happiness</td>\n",
       "      <td>World Happiness Report</td>\n",
       "      <td>Happiness scored according to economic product...</td>\n",
       "      <td>Sustainable Development Solutions Network</td>\n",
       "      <td>### Context \\n\\nThe World Happiness Report is ...</td>\n",
       "      <td>2</td>\n",
       "      <td>economics, social sciences, emotion</td>\n",
       "      <td>2017-06-14 20:41:45</td>\n",
       "      <td>CC0: Public Domain</td>\n",
       "      <td>29KB</td>\n",
       "      <td>29425</td>\n",
       "      <td>67658</td>\n",
       "      <td>6</td>\n",
       "      <td>273284</td>\n",
       "      <td>976</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uciml/iris</td>\n",
       "      <td>Iris Species</td>\n",
       "      <td>Classify iris plants into three species in thi...</td>\n",
       "      <td>UCI Machine Learning</td>\n",
       "      <td>The Iris dataset was used in R.A. Fisher's cla...</td>\n",
       "      <td>2</td>\n",
       "      <td>botany</td>\n",
       "      <td>2016-09-27 07:38:05</td>\n",
       "      <td>CC0: Public Domain</td>\n",
       "      <td>4KB</td>\n",
       "      <td>3718</td>\n",
       "      <td>65859</td>\n",
       "      <td>18</td>\n",
       "      <td>292620</td>\n",
       "      <td>984</td>\n",
       "      <td>3810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wendykan/lending-club-loan-data</td>\n",
       "      <td>Lending Club Loan Data</td>\n",
       "      <td>Analyze Lending Club's issued loans</td>\n",
       "      <td>Wendy Kan</td>\n",
       "      <td>These files contain complete loan data for all...</td>\n",
       "      <td>1</td>\n",
       "      <td>finance</td>\n",
       "      <td>2019-03-18 18:43:12</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>702MB</td>\n",
       "      <td>736483000</td>\n",
       "      <td>52096</td>\n",
       "      <td>33</td>\n",
       "      <td>293239</td>\n",
       "      <td>953</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>uciml/breast-cancer-wisconsin-data</td>\n",
       "      <td>Breast Cancer Wisconsin (Diagnostic) Data Set</td>\n",
       "      <td>Predict whether the cancer is benign or malignant</td>\n",
       "      <td>UCI Machine Learning</td>\n",
       "      <td>Features are computed from a digitized image o...</td>\n",
       "      <td>2</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>2016-09-25 10:49:04</td>\n",
       "      <td>CC BY-NC-SA 4.0</td>\n",
       "      <td>48KB</td>\n",
       "      <td>49196</td>\n",
       "      <td>51767</td>\n",
       "      <td>22</td>\n",
       "      <td>321503</td>\n",
       "      <td>837</td>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>uciml/pima-indians-diabetes-database</td>\n",
       "      <td>Pima Indians Diabetes Database</td>\n",
       "      <td>Predict the onset of diabetes based on diagnos...</td>\n",
       "      <td>UCI Machine Learning</td>\n",
       "      <td>## Context\\n\\nThis dataset is originally from ...</td>\n",
       "      <td>1</td>\n",
       "      <td>healthcare, india, health sciences</td>\n",
       "      <td>2016-10-06 18:31:56</td>\n",
       "      <td>CC0: Public Domain</td>\n",
       "      <td>9KB</td>\n",
       "      <td>9077</td>\n",
       "      <td>51210</td>\n",
       "      <td>11</td>\n",
       "      <td>268744</td>\n",
       "      <td>601</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mehdidag/black-friday</td>\n",
       "      <td>Black Friday</td>\n",
       "      <td>A study of sales trough consumer behaviours</td>\n",
       "      <td>Mehdi Dagdoug</td>\n",
       "      <td>## Description\\n\\nThe dataset here is a sample...</td>\n",
       "      <td>1</td>\n",
       "      <td>business, regression analysis</td>\n",
       "      <td>2018-07-25 20:49:48</td>\n",
       "      <td>CC0: Public Domain</td>\n",
       "      <td>5MB</td>\n",
       "      <td>5621145</td>\n",
       "      <td>49935</td>\n",
       "      <td>28</td>\n",
       "      <td>327920</td>\n",
       "      <td>1230</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>russellyates88/suicide-rates-overview-1985-to-...</td>\n",
       "      <td>Suicide Rates Overview 1985 to 2016</td>\n",
       "      <td>Compares socio-economic info with suicide rate...</td>\n",
       "      <td>Rusty</td>\n",
       "      <td>### Content\\n\\nThis compiled dataset pulled fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>economics, demographics, world</td>\n",
       "      <td>2018-12-01 19:18:25</td>\n",
       "      <td>World Bank Dataset Terms of Use</td>\n",
       "      <td>396KB</td>\n",
       "      <td>405202</td>\n",
       "      <td>47317</td>\n",
       "      <td>15</td>\n",
       "      <td>243420</td>\n",
       "      <td>1285</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>START-UMD/gtd</td>\n",
       "      <td>Global Terrorism Database</td>\n",
       "      <td>More than 180,000 terrorist attacks worldwide,...</td>\n",
       "      <td>START Consortium</td>\n",
       "      <td># Context \\n\\nInformation on more than 180,000...</td>\n",
       "      <td>3</td>\n",
       "      <td>crime, terrorism, international relations</td>\n",
       "      <td>2018-09-10 18:22:18</td>\n",
       "      <td>Other (specified in description)</td>\n",
       "      <td>28MB</td>\n",
       "      <td>29287587</td>\n",
       "      <td>45962</td>\n",
       "      <td>14</td>\n",
       "      <td>317522</td>\n",
       "      <td>1357</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>karangadiya/fifa19</td>\n",
       "      <td>FIFA 19 complete player dataset</td>\n",
       "      <td>18k+ FIFA 19 players, ~90 attributes extracted...</td>\n",
       "      <td>Karan Gadiya</td>\n",
       "      <td>### Context\\n\\nFootball analytics\\n\\n\\n### Con...</td>\n",
       "      <td>4</td>\n",
       "      <td>data visualization, feature engineering, rando...</td>\n",
       "      <td>2018-12-21 03:52:59</td>\n",
       "      <td>CC BY-NC-SA 4.0</td>\n",
       "      <td>2MB</td>\n",
       "      <td>2185152</td>\n",
       "      <td>44429</td>\n",
       "      <td>17</td>\n",
       "      <td>256144</td>\n",
       "      <td>1339</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>snap/amazon-fine-food-reviews</td>\n",
       "      <td>Amazon Fine Food Reviews</td>\n",
       "      <td>Analyze ~500,000 food reviews from Amazon</td>\n",
       "      <td>Stanford Network Analysis Project</td>\n",
       "      <td>## Context\\n\\nThis dataset consists of reviews...</td>\n",
       "      <td>2</td>\n",
       "      <td>internet, linguistics, food and drink</td>\n",
       "      <td>2017-05-01 18:51:31</td>\n",
       "      <td>CC0: Public Domain</td>\n",
       "      <td>251MB</td>\n",
       "      <td>262826594</td>\n",
       "      <td>44214</td>\n",
       "      <td>13</td>\n",
       "      <td>246703</td>\n",
       "      <td>755</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gregorut/videogamesales</td>\n",
       "      <td>Video Game Sales</td>\n",
       "      <td>Analyze sales data from more than 16,500 games.</td>\n",
       "      <td>GregorySmith</td>\n",
       "      <td>This dataset contains a list of video games wi...</td>\n",
       "      <td>2</td>\n",
       "      <td>video games</td>\n",
       "      <td>2016-10-26 09:10:49</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>403KB</td>\n",
       "      <td>412254</td>\n",
       "      <td>42459</td>\n",
       "      <td>13</td>\n",
       "      <td>183622</td>\n",
       "      <td>659</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mczielinski/bitcoin-historical-data</td>\n",
       "      <td>Bitcoin Historical Data</td>\n",
       "      <td>Bitcoin data at 1-min intervals from select ex...</td>\n",
       "      <td>Zielak</td>\n",
       "      <td>### Context \\nBitcoin is the longest running a...</td>\n",
       "      <td>16</td>\n",
       "      <td>finance, history</td>\n",
       "      <td>2019-03-15 16:22:58</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>118MB</td>\n",
       "      <td>123326534</td>\n",
       "      <td>42146</td>\n",
       "      <td>27</td>\n",
       "      <td>320438</td>\n",
       "      <td>1263</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>datasnaek/youtube-new</td>\n",
       "      <td>Trending YouTube Video Statistics</td>\n",
       "      <td>Daily statistics for trending YouTube videos</td>\n",
       "      <td>Mitchell J</td>\n",
       "      <td>UPDATE: Source code used for collecting this d...</td>\n",
       "      <td>115</td>\n",
       "      <td>internet, linguistics, languages, statistics, ...</td>\n",
       "      <td>2019-06-03 00:56:47</td>\n",
       "      <td>CC0: Public Domain</td>\n",
       "      <td>199MB</td>\n",
       "      <td>208756147</td>\n",
       "      <td>40618</td>\n",
       "      <td>25</td>\n",
       "      <td>257764</td>\n",
       "      <td>1083</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>harlfoxem/housesalesprediction</td>\n",
       "      <td>House Sales in King County, USA</td>\n",
       "      <td>Predict house price using regression</td>\n",
       "      <td>harlfoxem</td>\n",
       "      <td>This dataset contains house sale prices for Ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>finance, home</td>\n",
       "      <td>2016-08-25 15:52:49</td>\n",
       "      <td>CC0: Public Domain</td>\n",
       "      <td>778KB</td>\n",
       "      <td>796588</td>\n",
       "      <td>38553</td>\n",
       "      <td>18</td>\n",
       "      <td>221699</td>\n",
       "      <td>639</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ref  \\\n",
       "0                             mlg-ulb/creditcardfraud   \n",
       "1                            tmdb/tmdb-movie-metadata   \n",
       "2                                  hugomathien/soccer   \n",
       "3                       lava18/google-play-store-apps   \n",
       "4                               zynicide/wine-reviews   \n",
       "5                            ronitf/heart-disease-uci   \n",
       "6                              unsdsn/world-happiness   \n",
       "7                                          uciml/iris   \n",
       "8                     wendykan/lending-club-loan-data   \n",
       "9                  uciml/breast-cancer-wisconsin-data   \n",
       "10               uciml/pima-indians-diabetes-database   \n",
       "11                              mehdidag/black-friday   \n",
       "12  russellyates88/suicide-rates-overview-1985-to-...   \n",
       "13                                      START-UMD/gtd   \n",
       "14                                 karangadiya/fifa19   \n",
       "15                      snap/amazon-fine-food-reviews   \n",
       "16                            gregorut/videogamesales   \n",
       "17                mczielinski/bitcoin-historical-data   \n",
       "18                              datasnaek/youtube-new   \n",
       "19                     harlfoxem/housesalesprediction   \n",
       "\n",
       "                                            title  \\\n",
       "0                     Credit Card Fraud Detection   \n",
       "1                         TMDB 5000 Movie Dataset   \n",
       "2                        European Soccer Database   \n",
       "3                          Google Play Store Apps   \n",
       "4                                    Wine Reviews   \n",
       "5                               Heart Disease UCI   \n",
       "6                          World Happiness Report   \n",
       "7                                    Iris Species   \n",
       "8                          Lending Club Loan Data   \n",
       "9   Breast Cancer Wisconsin (Diagnostic) Data Set   \n",
       "10                 Pima Indians Diabetes Database   \n",
       "11                                   Black Friday   \n",
       "12           Suicide Rates Overview 1985 to 2016    \n",
       "13                      Global Terrorism Database   \n",
       "14                FIFA 19 complete player dataset   \n",
       "15                       Amazon Fine Food Reviews   \n",
       "16                               Video Game Sales   \n",
       "17                        Bitcoin Historical Data   \n",
       "18              Trending YouTube Video Statistics   \n",
       "19                House Sales in King County, USA   \n",
       "\n",
       "                                             subtitle  \\\n",
       "0   Anonymized credit card transactions labeled as...   \n",
       "1                 Metadata on ~5,000 movies from TMDb   \n",
       "2   25k+ matches, players & teams attributes for E...   \n",
       "3   Web scraped data of 10k Play Store apps for an...   \n",
       "4   130k wine reviews with variety, location, wine...   \n",
       "5   https://archive.ics.uci.edu/ml/datasets/Heart+...   \n",
       "6   Happiness scored according to economic product...   \n",
       "7   Classify iris plants into three species in thi...   \n",
       "8                 Analyze Lending Club's issued loans   \n",
       "9   Predict whether the cancer is benign or malignant   \n",
       "10  Predict the onset of diabetes based on diagnos...   \n",
       "11        A study of sales trough consumer behaviours   \n",
       "12  Compares socio-economic info with suicide rate...   \n",
       "13  More than 180,000 terrorist attacks worldwide,...   \n",
       "14  18k+ FIFA 19 players, ~90 attributes extracted...   \n",
       "15          Analyze ~500,000 food reviews from Amazon   \n",
       "16    Analyze sales data from more than 16,500 games.   \n",
       "17  Bitcoin data at 1-min intervals from select ex...   \n",
       "18       Daily statistics for trending YouTube videos   \n",
       "19               Predict house price using regression   \n",
       "\n",
       "                                      creator  \\\n",
       "0                Machine Learning Group - ULB   \n",
       "1                   The Movie Database (TMDb)   \n",
       "2                                Hugo Mathien   \n",
       "3                               Lavanya Gupta   \n",
       "4                                  zackthoutt   \n",
       "5                                       ronit   \n",
       "6   Sustainable Development Solutions Network   \n",
       "7                        UCI Machine Learning   \n",
       "8                                   Wendy Kan   \n",
       "9                        UCI Machine Learning   \n",
       "10                       UCI Machine Learning   \n",
       "11                              Mehdi Dagdoug   \n",
       "12                                      Rusty   \n",
       "13                           START Consortium   \n",
       "14                               Karan Gadiya   \n",
       "15          Stanford Network Analysis Project   \n",
       "16                               GregorySmith   \n",
       "17                                     Zielak   \n",
       "18                                 Mitchell J   \n",
       "19                                  harlfoxem   \n",
       "\n",
       "                                          description  version  \\\n",
       "0   Context\\n---------\\n\\nIt is important that cre...        3   \n",
       "1   ### Background\\nWhat can we say about the succ...        2   \n",
       "2   The ultimate Soccer database for data analysis...       10   \n",
       "3   ### Context\\n\\nWhile many public datasets (on ...        6   \n",
       "4   ### Context\\n\\nAfter watching [Somm](http://ww...        4   \n",
       "5   ### Context \\n\\nThis database contains 76 attr...        1   \n",
       "6   ### Context \\n\\nThe World Happiness Report is ...        2   \n",
       "7   The Iris dataset was used in R.A. Fisher's cla...        2   \n",
       "8   These files contain complete loan data for all...        1   \n",
       "9   Features are computed from a digitized image o...        2   \n",
       "10  ## Context\\n\\nThis dataset is originally from ...        1   \n",
       "11  ## Description\\n\\nThe dataset here is a sample...        1   \n",
       "12  ### Content\\n\\nThis compiled dataset pulled fr...        1   \n",
       "13  # Context \\n\\nInformation on more than 180,000...        3   \n",
       "14  ### Context\\n\\nFootball analytics\\n\\n\\n### Con...        4   \n",
       "15  ## Context\\n\\nThis dataset consists of reviews...        2   \n",
       "16  This dataset contains a list of video games wi...        2   \n",
       "17  ### Context \\nBitcoin is the longest running a...       16   \n",
       "18  UPDATE: Source code used for collecting this d...      115   \n",
       "19  This dataset contains house sale prices for Ki...        1   \n",
       "\n",
       "                                             keywords         last_updated  \\\n",
       "0                    finance, machine learning, crime  2018-03-23 01:17:27   \n",
       "1                                                film  2017-09-28 01:09:12   \n",
       "2                        europe, association football  2016-10-23 22:31:38   \n",
       "3   internet, video games, computer science, mobil...  2019-02-03 13:55:47   \n",
       "4                     food and drink, critical theory  2017-11-27 17:08:04   \n",
       "5   classification, binary classification, health,...  2018-06-25 11:33:56   \n",
       "6                 economics, social sciences, emotion  2017-06-14 20:41:45   \n",
       "7                                              botany  2016-09-27 07:38:05   \n",
       "8                                             finance  2019-03-18 18:43:12   \n",
       "9                                          healthcare  2016-09-25 10:49:04   \n",
       "10                 healthcare, india, health sciences  2016-10-06 18:31:56   \n",
       "11                      business, regression analysis  2018-07-25 20:49:48   \n",
       "12                     economics, demographics, world  2018-12-01 19:18:25   \n",
       "13          crime, terrorism, international relations  2018-09-10 18:22:18   \n",
       "14  data visualization, feature engineering, rando...  2018-12-21 03:52:59   \n",
       "15              internet, linguistics, food and drink  2017-05-01 18:51:31   \n",
       "16                                        video games  2016-10-26 09:10:49   \n",
       "17                                   finance, history  2019-03-15 16:22:58   \n",
       "18  internet, linguistics, languages, statistics, ...  2019-06-03 00:56:47   \n",
       "19                                      finance, home  2016-08-25 15:52:49   \n",
       "\n",
       "                                         license_name   size  size(bytes)  \\\n",
       "0   Database: Open Database, Contents: Database Co...   66MB     69155632   \n",
       "1                    Other (specified in description)    9MB      9747156   \n",
       "2   Database: Open Database, Contents: © Original ...   34MB     36121187   \n",
       "3                                             Unknown    2MB      2013348   \n",
       "4                                     CC BY-NC-SA 4.0   51MB     53336173   \n",
       "5                                    Reddit API Terms    3KB         3438   \n",
       "6                                  CC0: Public Domain   29KB        29425   \n",
       "7                                  CC0: Public Domain    4KB         3718   \n",
       "8                                             Unknown  702MB    736483000   \n",
       "9                                     CC BY-NC-SA 4.0   48KB        49196   \n",
       "10                                 CC0: Public Domain    9KB         9077   \n",
       "11                                 CC0: Public Domain    5MB      5621145   \n",
       "12                    World Bank Dataset Terms of Use  396KB       405202   \n",
       "13                   Other (specified in description)   28MB     29287587   \n",
       "14                                    CC BY-NC-SA 4.0    2MB      2185152   \n",
       "15                                 CC0: Public Domain  251MB    262826594   \n",
       "16                                            Unknown  403KB       412254   \n",
       "17                                       CC BY-SA 4.0  118MB    123326534   \n",
       "18                                 CC0: Public Domain  199MB    208756147   \n",
       "19                                 CC0: Public Domain  778KB       796588   \n",
       "\n",
       "    downloads  discussions    views  likes  kernels  \n",
       "0      131731           40  3052086   3323     2121  \n",
       "1       92202           53   630515   1601     1462  \n",
       "2       80922           95   638231   1875     1458  \n",
       "3       72020           37   440983   1861      313  \n",
       "4       68002           24   342243   1609     1948  \n",
       "5       67832           41   404892   2161      625  \n",
       "6       67658            6   273284    976      461  \n",
       "7       65859           18   292620    984     3810  \n",
       "8       52096           33   293239    953      582  \n",
       "9       51767           22   321503    837      862  \n",
       "10      51210           11   268744    601      599  \n",
       "11      49935           28   327920   1230      202  \n",
       "12      47317           15   243420   1285      200  \n",
       "13      45962           14   317522   1357      693  \n",
       "14      44429           17   256144   1339      228  \n",
       "15      44214           13   246703    755      416  \n",
       "16      42459           13   183622    659      324  \n",
       "17      42146           27   320438   1263      126  \n",
       "18      40618           25   257764   1083      166  \n",
       "19      38553           18   221699    639      581  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Kaggle Datasets.csv')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_subs</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>combo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>26256285</td>\n",
       "      <td>People who haven't pooped in 2019 yet, why are...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>People who haven t pooped in 2019 yet  why are...</td>\n",
       "      <td>People who haven t pooped in 2019 yet  why are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>26256285</td>\n",
       "      <td>Stan Lee has passed away at 95 years old</td>\n",
       "      <td>As many of you know today is day that many of ...</td>\n",
       "      <td>As many of you know today is day that many of ...</td>\n",
       "      <td>Stan Lee has passed away at 95 years old</td>\n",
       "      <td>As many of you know today is day that many of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home</td>\n",
       "      <td>10068</td>\n",
       "      <td>Beautiful Home :)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Beautiful Home</td>\n",
       "      <td>Beautiful Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home</td>\n",
       "      <td>10068</td>\n",
       "      <td>This was finished yesterday..</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>This was finished yesterday</td>\n",
       "      <td>This was finished yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>videos</td>\n",
       "      <td>22083867</td>\n",
       "      <td>This is what happens when one company owns doz...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>This is what happens when one company owns doz...</td>\n",
       "      <td>This is what happens when one company owns doz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit  subreddit_subs  \\\n",
       "0  AskReddit        26256285   \n",
       "1  AskReddit        26256285   \n",
       "2       Home           10068   \n",
       "3       Home           10068   \n",
       "4     videos        22083867   \n",
       "\n",
       "                                               title  \\\n",
       "0  People who haven't pooped in 2019 yet, why are...   \n",
       "1           Stan Lee has passed away at 95 years old   \n",
       "2                                  Beautiful Home :)   \n",
       "3                      This was finished yesterday..   \n",
       "4  This is what happens when one company owns doz...   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                      \n",
       "1  As many of you know today is day that many of ...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                                                      \n",
       "1  As many of you know today is day that many of ...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  People who haven t pooped in 2019 yet  why are...   \n",
       "1           Stan Lee has passed away at 95 years old   \n",
       "2                                  Beautiful Home      \n",
       "3                      This was finished yesterday     \n",
       "4  This is what happens when one company owns doz...   \n",
       "\n",
       "                                               combo  \n",
       "0  People who haven t pooped in 2019 yet  why are...  \n",
       "1  As many of you know today is day that many of ...  \n",
       "2                                  Beautiful Home     \n",
       "3                      This was finished yesterday    \n",
       "4  This is what happens when one company owns doz...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].str.replace('[^\\w\\s]',' ')\n",
    "df['clean_title'] = df['title'].str.replace('[^\\w\\s]',' ')\n",
    "df['combo'] = df['clean_text'] + df['clean_title']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[['subreddit', 'combo']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokens = []\n",
    "\n",
    "\"\"\" Make them tokens \"\"\"\n",
    "\n",
    "#stop words\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(['', ' ', '-', 'reddit', 'post'])\n",
    "    \n",
    "tokens = []\n",
    "\n",
    "for doc in tokenizer.pipe(df['combo'], batch_size=500):\n",
    "    \n",
    "    doc_tokens = []\n",
    "    \n",
    "    for token in doc: \n",
    "        if ((token.text.lower() not in STOP_WORDS) and \n",
    "            (token.is_stop == False) and \n",
    "            (token.is_punct == False) and \n",
    "            (token.pos_ != 'PRON')):\n",
    "                 doc_tokens.append(token.lemma_.lower())\n",
    "    tokens.append(' '.join(doc_tokens))\n",
    "    \n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.02, max_df=.98,\n",
    "                                   ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 257)\n",
      "(5000, 2501)\n"
     ]
    }
   ],
   "source": [
    "X = tfidf_vectorizer.fit_transform(df['tokens'])\n",
    "y = pd.get_dummies(df['subreddit']).values #target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(16, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(12, activation='relu'))\n",
    "model.add(keras.layers.Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 1s 179us/sample - loss: 0.0032 - acc: 0.9996\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(model.predict(X0), axis=1) == np.argmax(y0, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([921])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "º"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
